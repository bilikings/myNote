# 人工智能复习补充材料

##  名词解释

### 人工智能(Artificial Intelligence)：

 人工智能是研究、开发用于模拟、延伸和扩展人智能的理论、方法、技术及应用系统的一门新技术科学。 人工智能领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。智能的定义：智能是 个人从经验中学习、理性思考、记忆重要信息、以及应付日常生活需求的认知能力。

###  机器学习(Machine Learning) 

机器学习是一门专门研究计算机模拟或实现人类的学习行为，重新组织已有的知识结构，并获取新的知 识和技能，进而使之不断改善自身性能的学科。 

### 深度学习(Deep Learning) 

深度学习一般采用多层非线性变换，通过对数据特征的自动抽象和逐层提取，来实现分类或回归的学习 目的。 

### 物联网(IoT, Internet of things) 

物联网即“万物相连的互联网”，是互联网基础上的延伸和扩展的网络，将各种信息传感设备与互联网 结合起来而形成的一个巨大网络，实现在任何时间、任何地点，人、机、物的互联互通。 

### 专家系统(Expert system) 

专家系统是一个智能计算机程序系统，其内部含有大量的某个领域专家水平的知识与经验，能够利用人 类专家的知识和解决问题的方法来处理该领域问题。也就是说，专家系统是一个具有大量的专门知识与经验 的程序系统，它应用人工智能技术和计算机技术，根据某领域一个或多个专家提供的知识和经验，进行推理 和判断，模拟人类专家的决策过程，以便解决那些需要人类专家处理的复杂问题，简而言之，专家系统是一 种模拟人类专家解决领域问题的计算机程序系统。 

### 有监督学习(Supervised Learning) 

训练数据由一组训练实例组成。在监督学习中，每一个例子都是一对由一个输入对象（通常是一个向量） 和一个期望的输出值（也被称为监督信号）。有监督学习算法分析训练数据，并产生一个推断的功能，它可 以用于映射新的例子。一个最佳的方案将允许该算法正确地在标签不可见的情况下确定类标签。 用已知某种或某些特性的样本作为训练集，以建立一个数学模型(如模式识别中的判别模型，人工神经 网络法中的权重模型等)，再用已建立的模型来预测未知样本，此种方法称为有监督学习。是最常见的机器 学习方法。 

###  无监督学习(Unsupervised Learning) 

根据类别未知(没有被标记)的训练样本解决模式识别中的各种问题，称之为无监督学习。 不同于有监督学习，不需要用历史经验知识作为指导，，而是不断地自主从数据本身特性认知数据的特 征，自主归纳和巩固，形成对知识的认知结果。 

### 人工神经网络(ANN, Artificial Neural Network) 

也简称为神经网络(NN)，是模仿生物脑神经的工作原理，由大量简单的处理单元广泛互联形成复杂的 网络系统，从而对模糊的随机性的非线性数据表现出强大的逼近能力。 

### 代价函数(lost function)

(或者损失函数 loss function) 代价函数或损失函数是衡量学习模型的预测值与实际值之间误差的函数。这个误差被称为模型误差，函 数值越小表示误差越小，也就意味着预测值越接近实际值，认为所构建的模型性能越好。 二、各章节算法等信息补充 生成与测试范式 解决问题的直接方法是提出可能的解，然后检查每个提议，看是否有提议构成了解。 

## 算法与源码

### n皇后回溯解法

```java
class Solution {
        public List<List<String>> solveNQueens(int n) {
        char[][] chess = new char[n][n];
        for (int i = 0; i < n; i++)
            for (int j = 0; j < n; j++)
                chess[i][j] = '.';
        List<List<String>> res = new ArrayList<>();
        solve(res, chess, 0);
        return res;
    }

    private void solve(List<List<String>> res, char[][] chess, int row) {
        if (row == chess.length) {
            res.add(construct(chess));
            return;
        }
        for (int col = 0; col < chess.length; col++) {
            if (valid(chess, row, col)) {
                chess[row][col] = 'Q';
                solve(res, chess, row + 1);
                chess[row][col] = '.';
            }
        }
    }

    //row表示第几行，col表示第几列
    private boolean valid(char[][] chess, int row, int col) {
        //判断当前列有没有皇后,因为他是一行一行往下走的，
        //我们只需要检查走过的行数即可，通俗一点就是判断当前
        //坐标位置的上面有没有皇后
        for (int i = 0; i < row; i++) {
            if (chess[i][col] == 'Q') {
                return false;
            }
        }
        //判断当前坐标的右上角有没有皇后
        for (int i = row - 1, j = col + 1; i >= 0 && j < chess.length; i--, j++) {
            if (chess[i][j] == 'Q') {
                return false;
            }
        }
        //判断当前坐标的左上角有没有皇后
        for (int i = row - 1, j = col - 1; i >= 0 && j >= 0; i--, j--) {
            if (chess[i][j] == 'Q') {
                return false;
            }
        }
        return true;
    }

    //把数组转为list
    private List<String> construct(char[][] chess) {
        List<String> path = new ArrayList<>();
        for (int i = 0; i < chess.length; i++) {
            path.add(new String(chess[i]));
        }
        return path;
    }
}
```

### 贪婪(贪心算法)

 贪心算法(Greedy Algorithm)，又称贪婪算法，是一种**在每一步选择中都采取在当前状态下最好或最 优(即最有利)的选择**，从而希望导致结果是最好或最优的算法。比如在旅行销售员问题中，如果销售员每次 都选择最近的城市，那这就是一种贪心算法。Dijkstra 算法就算采用了贪心策略的实例，每次扩展最短的节 点，更新距离表。贪心算法不是对所有问题都能得到整体最优解，关键是贪心策略的选择。也就是说，不从 整体最优上加以考虑，做出的只是在某种意义上的局部最优解

###  TSP 旅行商问题(TSP 最短路径问题)

(Travelling Salesman Problem, TSP)是这样一个问题：给定一系列城 市和每对城市之间的距离，求解访问每一座城市一次并回到起始城市的最短回路。它是组合优化中的一个 NP Hard 问题，在运筹学和理论计算机科学中非常重要。TSP 的暴力解法时间复杂度是 O(n!)，目前现有的 非暴力算法只能无限逼近其最优解，而不能保证得到通解

### 盲目搜索

####  DFS

 实现借助 Stack/递归 在树很深、分支因子不大、解出现的位置较深的情况下适合 

#### BFS 

实现借助 Queue 在分支因子很大，解出现的位置合理深度、路径不是非常深的情况下适合

#### DFS-ID

迭代加深的深度优先搜索 实质上就是限定下界的深度优先搜索。即首先允许深度优先搜索 K 层搜索树，若没有发现可行解，再 将 K+1 后重复以上步骤搜索，直到搜索到可行解。在迭代加深搜索的算法中，连续的深度优先搜索被引入， 每一个深度约束逐次加 1，直到搜索到目标为止。迭代加深搜索算法就是仿广度优先搜索的深度优先搜索。 既能满足深度优先搜索的线性存储要求，又能保证发现一个最小深度的目标结点。从实际应用来看，迭代加 深搜索的效果比较好，并不比广度优先搜索慢很多，但是空间复杂度却与深度优先搜索相同，比广度优先搜 索小很多，在一些层次遍历的题目中，迭代加深不失为一种好方法 

### 知情搜索-分支定界法

 代价评估函数 f(n) = g(n) + h\*(n)表示从初始节点 S 到目标节点 n 的代价，g(n):从初始节点 S 到目标 节点 n 的距离，h*(n)从当前位置 n 经过最短路径到达 G 的剩余距离，然而在过程中无法直接求出 h*(n)， 因此使用一个估计值 h(n)代替，只有对所有节点 h(n)<=h*(n)才有意义，此时 h(n)被视为可接受的启发， 因此最终评估函数是 f(n) = g(n) + h(n) 

#### UCS 

一致代价搜索（普通的分支定界） 一致代价搜索的特点是 f(n)=g(n)，也就是说对任意 n 都有 h(n)=0，非常类似 BFS，不过 BFS 是努力 找一条通往目标的路径，而 UCS 是努力找一条最优路径。 

#### A*

 A\*算法实际上是综合 BFS、Dijkstra、最佳优先搜索算法的特点于一身的（或者说是采用了剩余距离估 计值和动态规划的分支定界法）。f(n)是节点 n 的综合优先级。当我们选择下一个要遍历的节点时，我们总 会选取综合优先级最高（值最小）的节点。g(n) 是节点 n 距离起点的代价。h(n)是节点 n 距离终点的预计 代价，这也就是 A*算法的启发函数。关于启发函数我们在下面详细讲解。A*算法在运算过程中，每次从优 先队列中选取 f(n)值最小（优先级最高）的节点作为下一个待遍历的节点。 另外，A*算法使用两个集合来表示待遍历的节点，与已经遍历过的节点，这通常称之为 open_set 和 close_set。 

完整A*算法过程

+ 初始化openSet和closeSet

+ 把起点加入openSet中

+ 如果openSet!=null,就从openSet中选择第一个节点n：

  + 如果n为终点

    就从双面开始逐步最终parent节点，一直到达起点，返回结果路径，算法结束

  + 如果n不是终点

    把节点n从openSet中移除，并加入到closeSet中

    遍历n节点的所有临近节点 

    + 如果临近节点在closeSet中，就跳过，选择下一个临近节点
    + 如果临近节点m不在openSet中
      1. 设置m的parent节点为n
      2. 计算m到终点的代价f（n）
      3. 把m节点加入到openSet中
    + 对openSet中的节点按照代价从小到大的排序（openSet可以使用优先队列优化）

+ 返回找不到路径的错误

#### 八数码

```java
class Solution {
    public int slidingPuzzle(int[][] board) {
        int R = board.length, C = board[0].length;
        int sr = 0, sc = 0;

        //Find sr, sc
        search://跳转
            for (sr = 0; sr < R; sr++)
                for (sc = 0; sc < C; sc++)
                    if (board[sr][sc] == 0)
                        break search;

        int[][] directions = new int[][]{{1, 0}, {-1, 0}, {0, 1}, {0, -1}};
        PriorityQueue<Node> heap = new PriorityQueue<Node>((a, b) ->
            (a.heuristic + a.depth) - (b.heuristic + b.depth));
        Node start = new Node(board, sr, sc, 0);
        heap.add(start);

        Map<String, Integer> cost = new HashMap();
        cost.put(start.boardstring, 9999999);

        String target = Arrays.deepToString(new int[][]{{1,2,3}, {4,5,0},{7,8,9}});//这里改成目标
        String targetWrong = Arrays.deepToString(new int[][]{{1,2,3}, {5,4,0},{7,8,9}});

        while (!heap.isEmpty()) {
            Node node = heap.poll();
            if (node.boardstring.equals(target))
                return node.depth;
            if (node.boardstring.equals(targetWrong))
                return -1;
            if (node.depth + node.heuristic > cost.get(node.boardstring))
                continue;

            for (int[] di: directions) {
                int nei_r = di[0] + node.zero_r;
                int nei_c = di[1] + node.zero_c;

                // 如果邻居不在版上，或者行列错误
                if ((Math.abs(nei_r - node.zero_r) + Math.abs(nei_c - node.zero_c) != 1) ||
                        nei_r < 0 || nei_r >= R || nei_c < 0 || nei_c >= C)
                    continue;

                int[][] newboard = new int[R][C];
                int t = 0;
                for (int[] row: node.board)
                    newboard[t++] = row.clone();

                // 在新版上交换元素
                newboard[node.zero_r][node.zero_c] = newboard[nei_r][nei_c];
                newboard[nei_r][nei_c] = 0;

                Node nei = new Node(newboard, nei_r, nei_c, node.depth+1);
                if (nei.depth + nei.heuristic >= cost.getOrDefault(nei.boardstring, 9999999))
                    continue;
                heap.add(nei);
                cost.put(nei.boardstring, nei.depth + nei.heuristic);
            }
        }

        return -1;
    }
}

class Node {
    int[][] board;
    String boardstring;
    int heuristic;
    int zero_r;
    int zero_c;
    int depth;
    Node(int[][] B, int zr, int zc, int d) {
        board = B;
        boardstring = Arrays.deepToString(board);

        //计算启发（heuristic）式
        heuristic = 0;
        int R = B.length, C = B[0].length;
        for (int r = 0; r < R; ++r)
            for (int c = 0; c < C; ++c) {
                if (board[r][c] == 0) continue;
                int v = (board[r][c] + R*C - 1) % (R*C);
                // v/C, v%C: board[r][c]应该到哪里的最短路径（用于评估函数
                ）
                heuristic += Math.abs(r - v/C) + Math.abs(c - v%C);
            }
        heuristic /= 2;
        zero_r = zr;
        zero_c = zc;
        depth = d;
    }
}

```

### 博弈树

 极大极小算法 python 实现(以解决 tic tac toe 井字棋问题为例) evaluate 方法功能是是对当前棋盘状态相对于 player 打分，分数越高对 player 越有利 

#### 极小极大算法

```java
public class MaxMinSearch {
    static final int INF = 0x3f3f3f3f;


    public int maxMinSearch(String graph, Set<Integer> vis, int len, int depth){//一维化二维平面x=graph.len/len;y=graph.len%len
        int bestValue,value;
        if(isGameOver()){//游戏结束，满足三连或者平局
            return evaluation(graph);//返回评估值
        }
        if(depth<=0){//达到叶子节点，返回估值，在井字棋中博弈树中深度为8
            return evaluation(graph);
        }
        if(ismax()){
            bestValue = -INF;
        }else {
            bestValue = INF;
        }
        for(int i =0;i<graph.length();i++){
            if(!vis.contains(i)){
                vis.add(i);//走棋
                value = maxMinSearch(graph,vis,len,depth-1);
                vis.remove(i);//回溯
                if(ismax()){
                    bestValue = Math.max(value, bestValue);
                }else {
                    bestValue = Math.min(value, bestValue);
                }
            }
        }
        return bestValue;
    }

    private boolean ismax() {
        return true;
    }

    private int evaluation(String graph) {
        return 0;
    }

    private boolean isGameOver() {
        return true;
    }
}
```

#### ab剪枝

```java
import java.util.Set;
public class ABMaxMinS {
    static final int INF = 0x3f3f3f3f;
    public int AlphaBeta(String graph, Set<Integer> vis,int len,int nPlay,int nA,int nB){
        if(isGameOver()){
            return evelution(graph);
        }
        if(nPlay==0){
            return evelution(graph);
        }
        if(isMinNode()){
            for (int i = 0; i < graph.length(); i++) {
                if(vis.contains(i)){
                    vis.add(i);
                    int score = AlphaBeta(graph, vis, len, nPlay-1, nA, nB);
                    vis.remove(i);
                    if(score<nB){
                        nB = score;
                        if(nA>=nB){
                            return nA;//alpha剪枝,选大的
                        }
                    }
                }
            }
        }else {
            for (int i = 0; i < graph.length(); i++) {
                if (vis.contains(i)) {
                    vis.add(i);
                    int score = AlphaBeta(graph, vis, len, nPlay - 1, nA, nB);
                    vis.remove(i);
                    if (score > nA) {
                        nA = score;
                        if (nB <= nA) {
                            return nB;//alpha剪枝，选小的
                        }
                    }
                }
            }
        }
        return nA;//返回最小的

    }

    private boolean isMinNode() {
        return true;
    }

    private boolean isGameOver() {
        return true;
    }

    private int evelution(String graph) {
        return 0;
    }
}
```

### 负极小值极大

```java
public class NegaMax {
    static final int INF = 0x3f3f3f3f;
    public int nageMax(String graph, Set<Integer> vis, int len, int depth){
        int best = -INF;
        if(isGameOver()){//游戏结束，满足三连或者平局
            return evaluation(graph);//返回评估值
        }
        if(depth<=0){
            return evaluation(graph);
        }
        for(int i =0;i<graph.length();i++){
            if(!vis.contains(i)){
                vis.add(i);//走棋
                int value = -nageMax(graph,vis,len,depth-1);//这里是负号
                vis.remove(i);//回溯
                if(value<best){
                    best = value;
                }
            }
        }
        return best;
    }

    private int evaluation(String graph) {
        return 0;
    }

    private boolean isGameOver() {
        return true;
    }
}
```

### 神经网络 

#### 神经网络神经元模型 

![image-20201026151434846](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20201026151434846.png)

#### 反向传播算法(Back Propagation) 

步骤：

1. 前向传播（正向数据传播） 样本从输入层输入网络，随机初始化权重和偏置，通过求和及激活函数计算，直到输出层输出结果 

   反向误差传播 计算前向传播阶段网络的输出与期望输出（样本标签）之间的误差，从输出层到隐含层，最后到输入 层，逐层调节各层每个神经元的连接权重和偏置 

